{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "#from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import glob\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from imgaug import augmenters as iaa\n",
    "import imgaug as ia\n",
    "import socket\n",
    "from datetime import datetime\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tqdm\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.nn import init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_path = 'E:\\Data\\CCF_Data\\Hemangioma'\n",
    "\n",
    "class DefaultConfig(object):\n",
    "    num_epochs=10\n",
    "    epoch_start_i=0\n",
    "    checkpoint_step=5\n",
    "    validation_step = 1\n",
    "    crop_height = 256\n",
    "    crop_width = 448\n",
    "    batch_size = 2\n",
    "    #训练集所在位置，根据自身训练集位置进行修改\n",
    "    data = 'train/'\n",
    "\n",
    "    log_dirs = os.path.join(ROOT_path, 'Log/OCT')\n",
    "    \n",
    "    lr=0.01\n",
    "    lr_mode= 'poly'\n",
    "    net_work= 'BaseNet'\n",
    "    #net_work= 'MSSeg'  #net_work= 'UNet'\n",
    "\n",
    "    momentum = 0.9#\n",
    "    weight_decay = 1e-4#\n",
    "\n",
    "\n",
    "    mode='train'\n",
    "    num_classes = 2\n",
    "\n",
    "    \n",
    "    k_fold=4\n",
    "    test_fold=4\n",
    "    num_workers=0\n",
    "    \n",
    "    cuda='0'\n",
    "    use_gpu=True\n",
    "    pretrained_model_path= os.path.join(ROOT_path, 'pretrained', 'resnet34-333f7ec4.pth')\n",
    "    save_model_path='./checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self,smooth=0.01):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "        \n",
    "    def forward(self,input, target):\n",
    "        input = torch.sigmoid(input)\n",
    "        Dice = Variable(torch.Tensor([0]).float()).cuda()\n",
    "        intersect=(input*target).sum()\n",
    "        union = torch.sum(input) + torch.sum(target)\n",
    "        Dice=(2*intersect+self.smooth)/(union+self.smooth)\n",
    "        dice_loss=1-Dice\n",
    "        return dice_loss\n",
    "\n",
    "class Multi_DiceLoss(nn.Module):\n",
    "    def __init__(self, class_num=5,smooth=0.001):\n",
    "        super(Multi_DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "        self.class_num = class_num\n",
    "\n",
    "    def forward(self,input, target):\n",
    "        input = torch.exp(input)\n",
    "        Dice = Variable(torch.Tensor([0]).float()).cuda()\n",
    "        for i in range(0,self.class_num):\n",
    "            input_i = input[:,i,:,:]\n",
    "            target_i = (target == i).float()\n",
    "            intersect = (input_i*target_i).sum()\n",
    "            union = torch.sum(input_i) + torch.sum(target_i)\n",
    "            dice = (2 * intersect + self.smooth) / (union + self.smooth)\n",
    "            Dice += dice\n",
    "        dice_loss = 1 - Dice/(self.class_num)\n",
    "        return dice_loss\n",
    "\n",
    "class EL_DiceLoss(nn.Module):\n",
    "    def __init__(self, class_num=4,smooth=1,gamma=0.5):\n",
    "        super(EL_DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "        self.class_num = class_num\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self,input, target):\n",
    "        input = torch.exp(input)\n",
    "        self.smooth = 0.\n",
    "        Dice = Variable(torch.Tensor([0]).float()).cuda()\n",
    "        for i in range(1,self.class_num):\n",
    "            input_i = input[:,i,:,:]\n",
    "            target_i = (target == i).float()\n",
    "            intersect = (input_i*target_i).sum()\n",
    "            union = torch.sum(input_i) + torch.sum(target_i)\n",
    "            if target_i.sum() == 0:\n",
    "                dice = Variable(torch.Tensor([1]).float()).cuda()\n",
    "            else:\n",
    "                dice = (2 * intersect + self.smooth) / (union + self.smooth)\n",
    "            Dice += (-torch.log(dice))**self.gamma\n",
    "        dice_loss = Dice/(self.class_num - 1)\n",
    "        return dice_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation():\n",
    "    # augment images with spatial transformation: Flip, Affine, Rotation, etc...\n",
    "    # see https://github.com/aleju/imgaug for more details\n",
    "    pass\n",
    "\n",
    "def augmentation_pixel():\n",
    "    # augment images with pixel intensity transformation: GaussianBlur, Multiply, etc...\n",
    "    pass\n",
    "\n",
    "class Data(torch.utils.data.Dataset):\n",
    "    Unlabelled=[0, 0, 0]\n",
    "    sick = [255, 255, 255]\n",
    "    COLOR_DICT = np.array([Unlabelled,sick])\n",
    "    def __init__(self, dataset_path,scale=(320, 320), mode='train'):\n",
    "        super().__init__()\n",
    "        self.mode = mode\n",
    "        self.img_path=dataset_path+'/img'\n",
    "        self.mask_path=dataset_path+'/mask'\n",
    "        self.image_lists, self.label_lists=self.read_list(self.img_path)\n",
    "        self.resize = scale\n",
    "        self.flip =iaa.SomeOf((2,5),[     \n",
    "             iaa.PiecewiseAffine(scale=(0, 0.1), nb_rows=4, nb_cols=4, cval=0),                  \n",
    "             iaa.Fliplr(0.5),\n",
    "             iaa.Flipud(0.1),         \n",
    "             iaa.Affine(rotate=(-20, 20),\n",
    "                        scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)}),\n",
    "             iaa.OneOf([\n",
    "                    iaa.GaussianBlur((0, 1.0)), # blur images with a sigma between 0 and 3.0\n",
    "                    iaa.AverageBlur(k=(3, 5)), # blur image using local means with kernel sizes between 2 and 7\n",
    "                    iaa.MedianBlur(k=(3, 5)), # blur image using local medians with kernel sizes between 2 and 7\n",
    "                ]),\n",
    "             iaa.contrast.LinearContrast((0.5, 1.5))],\n",
    "                              random_order=True)\n",
    "\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # load image and crop\n",
    "        img = Image.open(self.image_lists[index]).convert('RGB')\n",
    "        img = img.resize(self.resize)\n",
    "        img = np.array(img)\n",
    "        labels=self.label_lists[index]\n",
    "        #load label\n",
    "        if self.mode !='test':\n",
    "            label_ori = Image.open(self.label_lists[index]).convert('RGB')\n",
    "            label_ori = label_ori.resize(self.resize)\n",
    "            label_ori = np.array(label_ori)\n",
    "            label=np.ones(shape=(label_ori.shape[0],label_ori.shape[1]),dtype=np.uint8)\n",
    "\n",
    "            #convert RGB  to one hot\n",
    "            \n",
    "            for i in range(len(self.COLOR_DICT)):\n",
    "                equality = np.equal(label_ori, self.COLOR_DICT[i])\n",
    "                class_map = np.all(equality, axis=-1)\n",
    "                label[class_map]=i\n",
    "\n",
    "            # augment image and label\n",
    "            if self.mode == 'train':\n",
    "                seq_det = self.flip.to_deterministic()#固定变换\n",
    "                segmap = ia.SegmentationMapsOnImage(label, shape=label.shape)\n",
    "                img = seq_det.augment_image(img)\n",
    "                label = seq_det.augment_segmentation_maps([segmap])[0].get_arr().astype(np.uint8)\n",
    "\n",
    "            label_img=torch.from_numpy(label.copy()).float()\n",
    "            if self.mode == 'val':\n",
    "                img_num=len(os.listdir(os.path.dirname(labels)))\n",
    "                labels=label_img,img_num\n",
    "            else:\n",
    "                labels=label_img\n",
    "        imgs=img.transpose(2, 0, 1)/255.0\n",
    "        img = torch.from_numpy(imgs.copy()).float()#self.to_tensor(img.copy()).float()\n",
    "        return img, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_lists)\n",
    "\n",
    "    def read_list(self, image_path):\n",
    "        fold = os.listdir(image_path)\n",
    "        # fold = sorted(os.listdir(image_path), key=lambda x: int(x[-2:]))\n",
    "        # print(fold)\n",
    "\n",
    "        img_list = []\n",
    "        label_list = []\n",
    "        if self.mode=='train':\n",
    "            for item in fold:\n",
    "                name = item.split('.')[0]\n",
    "                img_list.append(os.path.join(image_path, item))\n",
    "                label_list.append(os.path.join(image_path.replace('img', 'mask'), '{}.png'.format(name)))\n",
    "\n",
    "\n",
    "        elif self.mode == 'val':\n",
    "            for item in fold:\n",
    "                name = item.split('.')[0]\n",
    "                img_list.append(os.path.join(image_path, item))\n",
    "                label_list.append(os.path.join(image_path.replace('img', 'mask'), '{}.png'.format(name)))\n",
    "\n",
    "\n",
    "        elif self.mode=='test':\n",
    "            for item in fold:\n",
    "                name = item.split('.')[0]\n",
    "                img_list.append(os.path.join(image_path, item))\n",
    "                label_list.append(os.path.join(image_path.replace('img', 'mask'), '{}.png'.format(name)))\n",
    "                \n",
    "        return img_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os.path as osp\n",
    "def save_checkpoint(state,best_pred, epoch,is_best,checkpoint_path, filename='./checkpoint/checkpoint.pth'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, osp.join(checkpoint_path,'best.pth'))\n",
    "    # shutil.copyfile(filename, osp.join(checkpoint_path, 'model_ep{}.pth'.format(epoch+1)))\n",
    "\n",
    "\n",
    "\n",
    "def adjust_learning_rate(opt, optimizer, epoch):                        \n",
    "    \"\"\"\n",
    "    Sets the learning rate to the initial LR decayed by 10 every 30 epochs(step = 30)\n",
    "    \"\"\"\n",
    "    if opt.lr_mode == 'step':\n",
    "        lr = opt.lr * (0.1 ** (epoch // opt.step))\n",
    "    elif opt.lr_mode == 'poly':\n",
    "        lr = opt.lr * (1 - epoch / opt.num_epochs) ** 0.9\n",
    "    else:\n",
    "        raise ValueError('Unknown lr mode {}'.format(opt.lr_mode))\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def one_hot_it(label, label_info):\n",
    "\t# return semantic_map -> [H, W, num_classes]\n",
    "\tsemantic_map = []\n",
    "\tfor info in label_info:\n",
    "\t\tcolor = label_info[info]\n",
    "\t\t# colour_map = np.full((label.shape[0], label.shape[1], label.shape[2]), colour, dtype=int)\n",
    "\t\tequality = np.equal(label, color)\n",
    "\t\tclass_map = np.all(equality, axis=-1)\n",
    "\t\tsemantic_map.append(class_map)\n",
    "\tsemantic_map = np.stack(semantic_map, axis=-1)\n",
    "\treturn semantic_map\n",
    "    \n",
    "    \n",
    "def compute_score(predict, target, forground = 1,smooth=1):\n",
    "    score = 0\n",
    "    count = 0\n",
    "    target[target!=forground]=0\n",
    "    predict[predict!=forground]=0\n",
    "    assert(predict.shape == target.shape)\n",
    "    overlap = ((predict == forground)*(target == forground)).sum() #TP\n",
    "    union=(predict == forground).sum() + (target == forground).sum()-overlap #FP+FN+TP\n",
    "    FP=(predict == forground).sum()-overlap #FP\n",
    "    FN=(target == forground).sum()-overlap #FN\n",
    "    TN= target.shape[0]*target.shape[1]-union #TN\n",
    "\n",
    "\n",
    "    #print('overlap:',overlap)\n",
    "    dice=(2*overlap +smooth)/ (union+overlap+smooth)\n",
    "    \n",
    "    precsion=((predict == target).sum()+smooth) / (target.shape[0]*target.shape[1]+smooth)\n",
    "    \n",
    "    jaccard=(overlap+smooth) / (union+smooth)\n",
    "\n",
    "    Sensitivity=(overlap+smooth) / ((target == forground).sum()+smooth)\n",
    "\n",
    "    Specificity=(TN+smooth) / (FP+TN+smooth)\n",
    "    \n",
    "\n",
    "    return dice,precsion,jaccard,Sensitivity,Specificity\n",
    "\n",
    "def eval_multi_seg(predict, target,num_classes):\n",
    "    # pred_seg=torch.argmax(torch.exp(predict),dim=1).int()\n",
    "    pred_seg = predict.data.cpu().numpy()\n",
    "    label_seg = target.data.cpu().numpy().astype(dtype=np.int)\n",
    "    assert(pred_seg.shape == label_seg.shape)\n",
    "    acc=(pred_seg==label_seg).sum()/(pred_seg.shape[0]*pred_seg.shape[1]*pred_seg.shape[2])\n",
    "    \n",
    "    # Dice = []\n",
    "    # Precsion = []\n",
    "    # Jaccard = []\n",
    "    # Sensitivity=[]\n",
    "    # Specificity=[]\n",
    "\n",
    "    # n = pred_seg.shape[0]\n",
    "    Dice=[]\n",
    "    True_label=[]\n",
    "    TP = FPN = 0\n",
    "    for classes in range(1,num_classes):\n",
    "        overlap=((pred_seg==classes)*(label_seg==classes)).sum()\n",
    "        union=(pred_seg==classes).sum()+(label_seg==classes).sum()\n",
    "        Dice.append((2*overlap+0.1)/(union+0.1))\n",
    "        True_label.append((label_seg==classes).sum())\n",
    "        TP += overlap\n",
    "        FPN +=union\n",
    "\n",
    "    \n",
    "    return Dice,True_label,acc, 2*TP/(FPN+1)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    # for i in range(n):\n",
    "    #     dice,precsion,jaccard,sensitivity,specificity= compute_score(pred_seg[i],label_seg[i])\n",
    "    #     Dice.append(dice)\n",
    "    #     Precsion .append(precsion)\n",
    "    #     Jaccard.append(jaccard)\n",
    "    #     Sensitivity.append(sensitivity)\n",
    "    #     Specificity.append(specificity)\n",
    "\n",
    "    # return Dice,Precsion,Jaccard,Sensitivity,Specificity\n",
    "\n",
    "\n",
    "def eval_seg(predict, target, forground = 1):\n",
    "    pred_seg=torch.round(torch.sigmoid(predict)).int()\n",
    "    pred_seg = pred_seg.data.cpu().numpy()\n",
    "    label_seg = target.data.cpu().numpy().astype(dtype=np.int)\n",
    "    assert(pred_seg.shape == label_seg.shape)\n",
    "\n",
    "    Dice = []\n",
    "    Precsion = []\n",
    "    Jaccard = []\n",
    "    n = pred_seg.shape[0]\n",
    "    \n",
    "    for i in range(n):\n",
    "        dice,precsion,jaccard = compute_score(pred_seg[i],label_seg[i])\n",
    "        Dice .append(dice)\n",
    "        Precsion .append(precsion)\n",
    "        Jaccard.append(jaccard)\n",
    "\n",
    "    return Dice,Precsion,Jaccard\n",
    "    \n",
    "\n",
    "def batch_pix_accuracy(pred,label,nclass=1):\n",
    "    if nclass==1:\n",
    "        pred=torch.round(torch.sigmoid(pred)).int()\n",
    "        pred=pred.cpu().numpy()\n",
    "    else:\n",
    "        pred=torch.max(pred,dim=1)\n",
    "        pred=pred.cpu().numpy()\n",
    "    label=label.cpu().numpy()\n",
    "    pixel_labeled = np.sum(label >=0)\n",
    "    pixel_correct=np.sum(pred==label)\n",
    "    \n",
    "    assert pixel_correct <= pixel_labeled, \\\n",
    "        \"Correct area should be smaller than Labeled\"\n",
    "    \n",
    "    return pixel_correct,pixel_labeled\n",
    "\n",
    "def batch_intersection_union(predict, target, nclass):\n",
    "\n",
    "    \"\"\"Batch Intersection of Union\n",
    "    Args:\n",
    "        predict: input 4D tensor\n",
    "        target: label 3D tensor\n",
    "        nclass: number of categories (int),note: not include background\n",
    "    \"\"\"\n",
    "    if nclass==1:\n",
    "        pred=torch.round(torch.sigmoid(predict)).int()\n",
    "        pred=pred.cpu().numpy()\n",
    "        target = target.cpu().numpy()\n",
    "        area_inter=np.sum(pred*target)\n",
    "        area_union=np.sum(pred)+np.sum(target)-area_inter\n",
    "\n",
    "        return area_inter,area_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.model_zoo as model_zoo\n",
    "import torchsummary\n",
    "\n",
    "\n",
    "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
    "           'resnet152']\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "}\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * Bottleneck.expansion, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * Bottleneck.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=1000, deep_base=False, stem_width=32):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.inplanes = stem_width*2 if deep_base else 64\n",
    "        if deep_base:\n",
    "            self.conv1= nn.Sequential(\n",
    "                nn.Conv2d(3, stem_width, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(stem_width),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(stem_width, stem_width, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(stem_width),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(stem_width, stem_width*2, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            )\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                                   bias=False)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        t = self.layer4(x)\n",
    "\n",
    "        # x = self.avgpool(x)\n",
    "        # x = x.view(x.size(0), -1)\n",
    "        # x = self.fc(x)\n",
    "\n",
    "        return t\n",
    "def resnet34(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-34 model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
    "    model_dict = model.state_dict()\n",
    "\n",
    "\n",
    "    if pretrained:\n",
    "        pretrained_dict=model_zoo.load_url(model_urls['resnet34'],model_dir='../pretrained')# Modify 'model_dir' according to your own path\n",
    "        print('Petrain Model Have been loaded!')\n",
    "        pretrained_dict =  {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "        model_dict.update(pretrained_dict)\n",
    "\n",
    "        model.load_state_dict(model_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_kwargs = {'mode': 'bilinear', 'align_corners': True}\n",
    "class GPG_3(nn.Module):\n",
    "    def __init__(self, in_channels, width=512, up_kwargs=None,norm_layer=nn.BatchNorm2d):\n",
    "        super(GPG_3, self).__init__()\n",
    "        self.up_kwargs = up_kwargs\n",
    "        \n",
    "\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels[-1], width, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(width),\n",
    "            nn.ReLU(inplace=True))\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels[-2], width, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(width),\n",
    "            nn.ReLU(inplace=True))\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels[-3], width, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(width),\n",
    "            nn.ReLU(inplace=True))\n",
    "        self.conv_out = nn.Sequential(\n",
    "            nn.Conv2d(3*width, width, 1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(width))\n",
    "        \n",
    "        self.dilation1 = nn.Sequential(SeparableConv2d(3*width, width, kernel_size=3, padding=1, dilation=1, bias=False),\n",
    "                                       nn.BatchNorm2d(width),\n",
    "                                       nn.ReLU(inplace=True))\n",
    "        self.dilation2 = nn.Sequential(SeparableConv2d(3*width, width, kernel_size=3, padding=2, dilation=2, bias=False),\n",
    "                                       nn.BatchNorm2d(width),\n",
    "                                       nn.ReLU(inplace=True))\n",
    "        self.dilation3 = nn.Sequential(SeparableConv2d(3*width, width, kernel_size=3, padding=4, dilation=4, bias=False),\n",
    "                                       nn.BatchNorm2d(width),\n",
    "                                       nn.ReLU(inplace=True))\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_uniform_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init.normal_(m.weight.data, 1.0, 0.02)\n",
    "                init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "    def forward(self, *inputs):\n",
    "        feats = [self.conv5(inputs[-1]), self.conv4(inputs[-2]), self.conv3(inputs[-3])]\n",
    "        _, _, h, w = feats[-1].size()\n",
    "        feats[-2] = F.interpolate(feats[-2], (h, w), **self.up_kwargs)\n",
    "        feats[-3] = F.interpolate(feats[-3], (h, w), **self.up_kwargs)\n",
    "        feat = torch.cat(feats, dim=1)\n",
    "        feat = torch.cat([self.dilation1(feat), self.dilation2(feat), self.dilation3(feat)], dim=1)\n",
    "        feat=self.conv_out(feat)\n",
    "        return feat\n",
    "class GPG_4(nn.Module):\n",
    "    def __init__(self, in_channels, width=512, up_kwargs=None,norm_layer=nn.BatchNorm2d):\n",
    "        super(GPG_4, self).__init__()\n",
    "        self.up_kwargs = up_kwargs\n",
    "        \n",
    "\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels[-1], width, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(width),\n",
    "            nn.ReLU(inplace=True))\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels[-2], width, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(width),\n",
    "            nn.ReLU(inplace=True))\n",
    "        self.conv_out = nn.Sequential(\n",
    "            nn.Conv2d(2*width, width, 1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(width))\n",
    "        \n",
    "        self.dilation1 = nn.Sequential(SeparableConv2d(2*width, width, kernel_size=3, padding=1, dilation=1, bias=False),\n",
    "                                       nn.BatchNorm2d(width),\n",
    "                                       nn.ReLU(inplace=True))\n",
    "        self.dilation2 = nn.Sequential(SeparableConv2d(2*width, width, kernel_size=3, padding=2, dilation=2, bias=False),\n",
    "                                       nn.BatchNorm2d(width),\n",
    "                                       nn.ReLU(inplace=True))\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_uniform_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init.normal_(m.weight.data, 1.0, 0.02)\n",
    "                init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "    def forward(self, *inputs):\n",
    "\n",
    "        feats = [self.conv5(inputs[-1]), self.conv4(inputs[-2])]\n",
    "        _, _, h, w = feats[-1].size()\n",
    "        feats[-2] = F.interpolate(feats[-2], (h, w), **self.up_kwargs)\n",
    "        feat = torch.cat(feats, dim=1)\n",
    "        feat = torch.cat([self.dilation1(feat), self.dilation2(feat)], dim=1)\n",
    "        feat=self.conv_out(feat)\n",
    "        return feat\n",
    "class GPG_2(nn.Module):\n",
    "    def __init__(self, in_channels, width=512, up_kwargs=None,norm_layer=nn.BatchNorm2d):\n",
    "        super(GPG_2, self).__init__()\n",
    "        self.up_kwargs = up_kwargs\n",
    "        \n",
    "\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels[-1], width, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(width),\n",
    "            nn.ReLU(inplace=True))\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels[-2], width, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(width),\n",
    "            nn.ReLU(inplace=True))\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels[-3], width, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(width),\n",
    "            nn.ReLU(inplace=True))\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels[-4], width, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(width),\n",
    "            nn.ReLU(inplace=True))\n",
    "\n",
    "        self.conv_out = nn.Sequential(\n",
    "            nn.Conv2d(4*width, width, 1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(width))\n",
    "        \n",
    "        self.dilation1 = nn.Sequential(SeparableConv2d(4*width, width, kernel_size=3, padding=1, dilation=1, bias=False),\n",
    "                                       nn.BatchNorm2d(width),\n",
    "                                       nn.ReLU(inplace=True))\n",
    "        self.dilation2 = nn.Sequential(SeparableConv2d(4*width, width, kernel_size=3, padding=2, dilation=2, bias=False),\n",
    "                                       nn.BatchNorm2d(width),\n",
    "                                       nn.ReLU(inplace=True))\n",
    "        self.dilation3 = nn.Sequential(SeparableConv2d(4*width, width, kernel_size=3, padding=4, dilation=4, bias=False),\n",
    "                                       nn.BatchNorm2d(width),\n",
    "                                       nn.ReLU(inplace=True))\n",
    "        self.dilation4 = nn.Sequential(SeparableConv2d(4*width, width, kernel_size=3, padding=8, dilation=8, bias=False),\n",
    "                                       nn.BatchNorm2d(width),\n",
    "                                       nn.ReLU(inplace=True))\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_uniform_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init.normal_(m.weight.data, 1.0, 0.02)\n",
    "                init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "    def forward(self, *inputs):\n",
    "\n",
    "        feats = [self.conv5(inputs[-1]), self.conv4(inputs[-2]),self.conv3(inputs[-3]),self.conv2(inputs[-4])]\n",
    "        _, _, h, w = feats[-1].size()\n",
    "        feats[-2] = F.interpolate(feats[-2], (h, w), **self.up_kwargs)\n",
    "        feats[-3] = F.interpolate(feats[-3], (h, w), **self.up_kwargs)\n",
    "        feats[-4] = F.interpolate(feats[-4], (h, w), **self.up_kwargs)\n",
    "        feat = torch.cat(feats, dim=1)\n",
    "        feat = torch.cat([self.dilation1(feat), self.dilation2(feat), self.dilation3(feat), self.dilation4(feat)], dim=1)\n",
    "        feat=self.conv_out(feat)\n",
    "        return feat\n",
    "\n",
    "class BaseNetHead(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, scale,\n",
    "                 is_aux=False, norm_layer=nn.BatchNorm2d):\n",
    "        super(BaseNetHead, self).__init__()\n",
    "        if is_aux:\n",
    "            self.conv_1x1_3x3=nn.Sequential(\n",
    "                ConvBnRelu(in_planes, 64, 1, 1, 0,\n",
    "                                       has_bn=True, norm_layer=norm_layer,\n",
    "                                       has_relu=True, has_bias=False),\n",
    "                ConvBnRelu(64, 64, 3, 1, 1,\n",
    "                                       has_bn=True, norm_layer=norm_layer,\n",
    "                                       has_relu=True, has_bias=False))\n",
    "        else:\n",
    "            self.conv_1x1_3x3=nn.Sequential(\n",
    "                ConvBnRelu(in_planes, 32, 1, 1, 0,\n",
    "                                       has_bn=True, norm_layer=norm_layer,\n",
    "                                       has_relu=True, has_bias=False),\n",
    "                ConvBnRelu(32, 32, 3, 1, 1,\n",
    "                                       has_bn=True, norm_layer=norm_layer,\n",
    "                                       has_relu=True, has_bias=False))\n",
    "        # self.dropout = nn.Dropout(0.1)\n",
    "        if is_aux:\n",
    "            self.conv_1x1_2 = nn.Conv2d(64, out_planes, kernel_size=1,\n",
    "                                      stride=1, padding=0)\n",
    "        else:\n",
    "            self.conv_1x1_2 = nn.Conv2d(32, out_planes, kernel_size=1,\n",
    "                                      stride=1, padding=0)\n",
    "        self.scale = scale\n",
    "            \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_uniform_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init.normal_(m.weight.data, 1.0, 0.02)\n",
    "                init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if self.scale > 1:\n",
    "            x = F.interpolate(x, scale_factor=self.scale,\n",
    "                                   mode='bilinear',\n",
    "                                   align_corners=True)\n",
    "        fm = self.conv_1x1_3x3(x)\n",
    "        # fm = self.dropout(fm)\n",
    "        output = self.conv_1x1_2(fm)\n",
    "        return output\n",
    "class SAPblock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(SAPblock, self).__init__()\n",
    "        self.conv3x3=nn.Conv2d(in_channels=in_channels, out_channels=in_channels,dilation=1,kernel_size=3, padding=1)\n",
    "        \n",
    "        self.bn=nn.ModuleList([nn.BatchNorm2d(in_channels),nn.BatchNorm2d(in_channels),nn.BatchNorm2d(in_channels)]) \n",
    "        self.conv1x1=nn.ModuleList([nn.Conv2d(in_channels=2*in_channels, out_channels=in_channels,dilation=1,kernel_size=1, padding=0),\n",
    "                                    nn.Conv2d(in_channels=2*in_channels, out_channels=in_channels,dilation=1,kernel_size=1, padding=0)])\n",
    "        self.conv3x3_1=nn.ModuleList([nn.Conv2d(in_channels=in_channels, out_channels=in_channels//2,dilation=1,kernel_size=3, padding=1),\n",
    "                                      nn.Conv2d(in_channels=in_channels, out_channels=in_channels//2,dilation=1,kernel_size=3, padding=1)])\n",
    "        self.conv3x3_2=nn.ModuleList([nn.Conv2d(in_channels=in_channels//2, out_channels=2,dilation=1,kernel_size=3, padding=1),\n",
    "                                      nn.Conv2d(in_channels=in_channels//2, out_channels=2,dilation=1,kernel_size=3, padding=1)])\n",
    "        self.conv_last=ConvBnRelu(in_planes=in_channels,out_planes=in_channels,ksize=1,stride=1,pad=0,dilation=1)\n",
    "\n",
    "\n",
    "\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "    \n",
    "        self.relu=nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x_size= x.size()\n",
    "\n",
    "        branches_1=self.conv3x3(x)\n",
    "        branches_1=self.bn[0](branches_1)\n",
    "\n",
    "        branches_2=F.conv2d(x,self.conv3x3.weight,padding=2,dilation=2)#share weight\n",
    "        branches_2=self.bn[1](branches_2)\n",
    "\n",
    "        branches_3=F.conv2d(x,self.conv3x3.weight,padding=4,dilation=4)#share weight\n",
    "        branches_3=self.bn[2](branches_3)\n",
    "\n",
    "        feat=torch.cat([branches_1,branches_2],dim=1)\n",
    "        # feat=feat_cat.detach()\n",
    "        feat=self.relu(self.conv1x1[0](feat))\n",
    "        feat=self.relu(self.conv3x3_1[0](feat))\n",
    "        att=self.conv3x3_2[0](feat)\n",
    "        att = F.softmax(att, dim=1)\n",
    "        \n",
    "        att_1=att[:,0,:,:].unsqueeze(1)\n",
    "        att_2=att[:,1,:,:].unsqueeze(1)\n",
    "\n",
    "        fusion_1_2=att_1*branches_1+att_2*branches_2\n",
    "\n",
    "\n",
    "\n",
    "        feat1=torch.cat([fusion_1_2,branches_3],dim=1)\n",
    "        # feat=feat_cat.detach()\n",
    "        feat1=self.relu(self.conv1x1[0](feat1))\n",
    "        feat1=self.relu(self.conv3x3_1[0](feat1))\n",
    "        att1=self.conv3x3_2[0](feat1)\n",
    "        att1 = F.softmax(att1, dim=1)\n",
    "        \n",
    "        att_1_2=att1[:,0,:,:].unsqueeze(1)\n",
    "        att_3=att1[:,1,:,:].unsqueeze(1)\n",
    "\n",
    "\n",
    "        ax=self.relu(self.gamma*(att_1_2*fusion_1_2+att_3*branches_3)+(1-self.gamma)*x)\n",
    "        ax=self.conv_last(ax)\n",
    "\n",
    "        return ax\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes,\n",
    "                 norm_layer=nn.BatchNorm2d,scale=2,relu=True,last=False):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "       \n",
    "\n",
    "        self.conv_3x3 = ConvBnRelu(in_planes, in_planes, 3, 1, 1,\n",
    "                                   has_bn=True, norm_layer=norm_layer,\n",
    "                                   has_relu=True, has_bias=False)\n",
    "        self.conv_1x1 = ConvBnRelu(in_planes, out_planes, 1, 1, 0,\n",
    "                                   has_bn=True, norm_layer=norm_layer,\n",
    "                                   has_relu=True, has_bias=False)\n",
    "       \n",
    "        self.sap=SAPblock(in_planes)\n",
    "        self.scale=scale\n",
    "        self.last=last\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_uniform_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init.normal_(m.weight.data, 1.0, 0.02)\n",
    "                init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if self.last==False:\n",
    "            x = self.conv_3x3(x)\n",
    "            # x=self.sap(x)\n",
    "        if self.scale>1:\n",
    "            x=F.interpolate(x,scale_factor=self.scale,mode='bilinear',align_corners=True)\n",
    "        x=self.conv_1x1(x)\n",
    "        return x\n",
    "\n",
    "class SeparableConv2d(nn.Module):\n",
    "    def __init__(self, inplanes, planes, kernel_size=3, stride=1, padding=1, dilation=1, bias=False, BatchNorm=nn.BatchNorm2d):\n",
    "        super(SeparableConv2d, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(inplanes, inplanes, kernel_size, stride, padding, dilation, groups=inplanes, bias=bias)\n",
    "        self.bn = BatchNorm(inplanes)\n",
    "        self.pointwise = nn.Conv2d(inplanes, planes, 1, 1, 0, 1, 1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x\n",
    "    \n",
    "class ConvBnRelu(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, ksize, stride, pad, dilation=1,\n",
    "                 groups=1, has_bn=True, norm_layer=nn.BatchNorm2d,\n",
    "                 has_relu=True, inplace=True, has_bias=False):\n",
    "        super(ConvBnRelu, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=ksize,\n",
    "                              stride=stride, padding=pad,\n",
    "                              dilation=dilation, groups=groups, bias=has_bias)\n",
    "        self.has_bn = has_bn\n",
    "        if self.has_bn:\n",
    "            self.bn = nn.BatchNorm2d(out_planes)\n",
    "        self.has_relu = has_relu\n",
    "        if self.has_relu:\n",
    "            self.relu = nn.ReLU(inplace=inplace)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.has_bn:\n",
    "            x = self.bn(x)\n",
    "        if self.has_relu:\n",
    "            x = self.relu(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class GlobalAvgPool2d(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"Global average pooling over the input's spatial dimensions\"\"\"\n",
    "        super(GlobalAvgPool2d, self).__init__()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        in_size = inputs.size()\n",
    "        inputs = inputs.view((in_size[0], in_size[1], -1)).mean(dim=2)\n",
    "        inputs = inputs.view(in_size[0], in_size[1], 1, 1)\n",
    "\n",
    "        return inputs\n",
    "class CPFNet(nn.Module):\n",
    "    def __init__(self,out_planes=1,ccm=True,norm_layer=nn.BatchNorm2d,is_training=True,expansion=2,base_channel=32):\n",
    "        super(CPFNet,self).__init__()\n",
    "        \n",
    "        self.backbone =resnet34(pretrained=False)\n",
    "        self.expansion=expansion\n",
    "        self.base_channel=base_channel\n",
    "        if self.expansion==4 and self.base_channel==64:\n",
    "            expan=[512,1024,2048]\n",
    "            spatial_ch=[128,256]\n",
    "        elif self.expansion==4 and self.base_channel==32:\n",
    "            expan=[256,512,1024]\n",
    "            spatial_ch=[32,128]\n",
    "            conv_channel_up=[256,384,512]\n",
    "        elif self.expansion==2 and self.base_channel==32:\n",
    "            expan=[128,256,512]\n",
    "            spatial_ch=[64,64]\n",
    "            conv_channel_up=[128,256,512]\n",
    "      \n",
    "        conv_channel = expan[0]\n",
    "        self.is_training = is_training\n",
    "        self.sap=SAPblock(expan[-1])\n",
    "\n",
    "        self.decoder5=DecoderBlock(expan[-1],expan[-2],relu=False,last=True) #256\n",
    "        self.decoder4=DecoderBlock(expan[-2],expan[-3],relu=False) #128\n",
    "        self.decoder3=DecoderBlock(expan[-3],spatial_ch[-1],relu=False) #64\n",
    "        self.decoder2=DecoderBlock(spatial_ch[-1],spatial_ch[-2]) #32\n",
    "        self.mce_2=GPG_2([spatial_ch[-1],expan[0], expan[1], expan[2]],width=spatial_ch[-1], up_kwargs=up_kwargs)\n",
    "        self.mce_3=GPG_3([expan[0], expan[1], expan[2]],width=expan[0], up_kwargs=up_kwargs)\n",
    "        self.mce_4=GPG_4([expan[1], expan[2]],width=expan[1], up_kwargs=up_kwargs)\n",
    "\n",
    "        self.main_head= BaseNetHead(spatial_ch[0], out_planes, 2,\n",
    "                             is_aux=False, norm_layer=norm_layer)\n",
    "       \n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.backbone.conv1(x)\n",
    "        x = self.backbone.bn1(x)\n",
    "        c1 = self.backbone.relu(x)#1/2  64\n",
    "        x = self.backbone.maxpool(c1)\n",
    "        c2 = self.backbone.layer1(x)#1/4   64\n",
    "        c3 = self.backbone.layer2(c2)#1/8   128\n",
    "        c4 = self.backbone.layer3(c3)#1/16   256\n",
    "        c5 = self.backbone.layer4(c4)#1/32   512\n",
    "        m2=self.mce_2(c2,c3,c4,c5)\n",
    "        m3=self.mce_3(c3,c4,c5)\n",
    "        m4=self.mce_4(c4,c5)\n",
    "        c5=self.sap(c5)\n",
    "        d4=self.relu(self.decoder5(c5)+m4)  #256\n",
    "        d3=self.relu(self.decoder4(d4)+m3)  #128\n",
    "        d2=self.relu(self.decoder3(d3)+m2) #64\n",
    "        d1=self.decoder2(d2)+c1 #32\n",
    "        main_out=self.main_head(d1)\n",
    "        main_out=F.log_softmax(main_out, dim=1)\n",
    "        return main_out, main_out\n",
    "    \n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_uniform_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init.normal_(m.weight.data, 1.0, 0.02)\n",
    "                init.constant_(m.bias.data, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(args, model, dataloader):\n",
    "    print('\\n')\n",
    "    print('Start Validation!')\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        tbar = tqdm.tqdm(dataloader, desc='\\r')\n",
    "       \n",
    "\n",
    "        total_Dice=[]\n",
    "        total_Dice1=[]\n",
    "        total_Dice2=[]\n",
    "        total_Dice3=[]\n",
    "        total_Dice.append(total_Dice1)\n",
    "        total_Dice.append(total_Dice2)\n",
    "        total_Dice.append(total_Dice3)\n",
    "        Acc=[]\n",
    "        \n",
    "        cur_cube=[]\n",
    "        cur_label_cube=[]\n",
    "        next_cube=[]\n",
    "        counter=0\n",
    "        end_flag=False\n",
    "\n",
    "        for i, (data, labels) in enumerate(tbar):\n",
    "            # tbar.update()\n",
    "            if torch.cuda.is_available() and args.use_gpu:\n",
    "                data = data.cuda()\n",
    "                label = labels[0].cuda()\n",
    "            slice_num=labels[1][0].long().item()\n",
    "            \n",
    "            # get RGB predict image\n",
    "            \n",
    "            aux_predict,predicts = model(data)\n",
    "            \n",
    "            predict=torch.argmax(torch.exp(predicts),dim=1)\n",
    "            batch_size=predict.size()[0]\n",
    "\n",
    "            counter+=batch_size\n",
    "            if counter<=slice_num:\n",
    "                cur_cube.append(predict)\n",
    "                cur_label_cube.append(label)\n",
    "                if counter==slice_num:\n",
    "                    end_flag=True\n",
    "                    counter=0\n",
    "            else:\n",
    "                last=batch_size-(counter-slice_num)\n",
    "\n",
    "                last_p=predict[0:last]\n",
    "                last_l=label[0:last]\n",
    "\n",
    "                first_p=predict[last:]\n",
    "                first_l=label[last:]\n",
    "\n",
    "                cur_cube.append(last_p)\n",
    "                cur_label_cube.append(last_l)\n",
    "                end_flag=True\n",
    "                counter=counter-slice_num\n",
    "\n",
    "            if end_flag:\n",
    "                end_flag=False\n",
    "                predict_cube=torch.stack(cur_cube,dim=0).squeeze()\n",
    "                label_cube=torch.stack(cur_label_cube,dim=0).squeeze()\n",
    "                cur_cube=[]\n",
    "                cur_label_cube=[]\n",
    "                if counter!=0:\n",
    "                    cur_cube.append(first_p)\n",
    "                    cur_label_cube.append(first_l)\n",
    "\n",
    "                assert predict_cube.size()[0]==slice_num\n",
    "                Dice,true_label,acc, mean_dice = eval_multi_seg(predict_cube,label_cube,args.num_classes)\n",
    "\n",
    "                for class_id in range(args.num_classes-1):\n",
    "                    if true_label[class_id]!=0:\n",
    "                        total_Dice[class_id].append(Dice[class_id])\n",
    "                Acc.append(acc)\n",
    "                len0=len(total_Dice[0]) if len(total_Dice[0])!=0 else 1\n",
    "                len1=len(total_Dice[1]) if len(total_Dice[1])!=0 else 1\n",
    "                len2=len(total_Dice[2]) if len(total_Dice[2])!=0 else 1\n",
    "\n",
    "                dice1=sum(total_Dice[0])/len0\n",
    "                dice2=sum(total_Dice[1])/len1\n",
    "                dice3=sum(total_Dice[2])/len2\n",
    "                ACC=sum(Acc)/len(Acc)\n",
    "                tbar.set_description('Mean_D: %3f, Dice1: %.3f, Dice2: %.3f, Dice3: %.3f, ACC: %.3f' % (mean_dice,dice1,dice2,dice3,ACC))\n",
    "        print('Mean_Dice:', mean_dice)\n",
    "        print('Dice1:',dice1)\n",
    "        print('Dice2:',dice2)\n",
    "        print('Dice3:',dice3)\n",
    "        print('Acc:',ACC)\n",
    "        return mean_dice,dice1,dice2,dice3,ACC\n",
    "\n",
    "\n",
    "def train(args, model, optimizer,criterion, dataloader_train, dataloader_val,):\n",
    "    current_time = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "    log_dir = os.path.join(args.log_dirs, current_time + '_' + socket.gethostname())\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "    step = 0\n",
    "    best_pred=0.0\n",
    "    for epoch in range(args.num_epochs):\n",
    "        lr = adjust_learning_rate(args,optimizer,epoch) \n",
    "        model.train()\n",
    "        tq = tqdm.tqdm(total=len(dataloader_train) * args.batch_size)\n",
    "        tq.set_description('epoch %d, lr %f' % (epoch, lr))\n",
    "        loss_record = []\n",
    "        train_loss=0.0\n",
    "#        is_best=False\n",
    "        for i,(data, label) in enumerate(dataloader_train):\n",
    "            # if i>9:\n",
    "            #     break\n",
    "            if torch.cuda.is_available() and args.use_gpu:\n",
    "                data = data.cuda()\n",
    "                label = label.cuda().long()\n",
    "            optimizer.zero_grad()\n",
    "            aux_out,main_out = model(data)\n",
    "            loss_aux=F.nll_loss(aux_out,label,weight=None)\n",
    "            loss_main= criterion[1](main_out, label)\n",
    "            loss =loss_aux+loss_main\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tq.update(args.batch_size)\n",
    "            train_loss += loss.item()\n",
    "            tq.set_postfix(loss='%.6f' % (train_loss/(i+1)))\n",
    "            step += 1\n",
    "            if step%10==0:\n",
    "                writer.add_scalar('Train/loss_step', loss, step)\n",
    "            loss_record.append(loss.item())\n",
    "        tq.close()\n",
    "        loss_train_mean = np.mean(loss_record)\n",
    "        writer.add_scalar('Train/loss_epoch', float(loss_train_mean), epoch)\n",
    "        print('loss for train : %f' % (loss_train_mean))\n",
    "        \n",
    "\n",
    "        if epoch % args.validation_step == 0:\n",
    "            mean_Dice,Dice1,Dice2,Dice3,acc= val(args, model, dataloader_val)\n",
    "            writer.add_scalar('Valid/Mean_val', mean_Dice, epoch)\n",
    "            writer.add_scalar('Valid/Dice1_val', Dice1, epoch)\n",
    "            writer.add_scalar('Valid/Dice2_val', Dice2, epoch)\n",
    "            writer.add_scalar('Valid/Dice3_val', Dice3, epoch)\n",
    "            writer.add_scalar('Valid/Acc_val', acc, epoch)\n",
    "            # mean_Dice=(Dice1+Dice2+Dice3)/3.0\n",
    "            is_best=mean_Dice > best_pred\n",
    "            best_pred = max(best_pred, mean_Dice)\n",
    "            print(best_pred)\n",
    "            checkpoint_dir = args.save_model_path\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir)\n",
    "            checkpoint_latest =os.path.join(checkpoint_dir, 'checkpoint_latest.pth')\n",
    "            save_checkpoint({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'best_dice': best_pred,\n",
    "                    }, best_pred,epoch,is_best, checkpoint_dir, filename=checkpoint_latest)\n",
    "                    \n",
    "def test(model,dataloader, args, save_path):\n",
    "    print('start test!')\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        # precision_record = []\n",
    "        tq = tqdm.tqdm(dataloader,desc='\\r')\n",
    "        tq.set_description('test')\n",
    "        comments=os.getcwd().split('\\\\')[-1]\n",
    "        for i, (data, label_path) in enumerate(tq):\n",
    "            if torch.cuda.is_available() and args.use_gpu:\n",
    "                data = data.cuda()\n",
    "                # label = label.cuda()\n",
    "            aux_pred,predict = model(data)\n",
    "            predict=torch.argmax(torch.exp(predict),dim=1)\n",
    "            pred=predict.data.cpu().numpy()\n",
    "            pred_RGB=Data.COLOR_DICT[pred.astype(np.uint8)]\n",
    "            \n",
    "            for index,item in enumerate(label_path):\n",
    "                img=Image.fromarray(pred_RGB[index].squeeze().astype(np.uint8))\n",
    "                _, name = os.path.split(item)\n",
    "\n",
    "                img.save(os.path.join(save_path, name))\n",
    "                # tq.set_postfix(str=str(save_img_path))\n",
    "        tq.close()\n",
    "            \n",
    "def main(mode='train',args=None):\n",
    "\n",
    "\n",
    "    # create dataset and dataloader\n",
    "    dataset_path = args.data\n",
    "    dataset_train = Data(os.path.join(dataset_path,'train'), scale=(args.crop_width, args.crop_height),mode='train')\n",
    "    dataloader_train = DataLoader(\n",
    "        dataset_train,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    dataset_val = Data(os.path.join(dataset_path, 'val'), scale=(args.crop_height, args.crop_width),mode='val')\n",
    "    dataloader_val = DataLoader(\n",
    "        dataset_val,\n",
    "        # this has to be 1\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    dataset_test=Data(os.path.join(dataset_path,'test'), scale=(args.crop_height, args.crop_width),mode='test')\n",
    "    dataloader_test = DataLoader(\n",
    "        dataset_test,\n",
    "        # this has to be 1\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    # build model\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args.cuda\n",
    "    \n",
    "    \n",
    "    #load model\n",
    "    model_all={'BaseNet': CPFNet(out_planes=args.num_classes)}\n",
    "    model=model_all[args.net_work]\n",
    "    print(args.net_work)\n",
    "    cudnn.benchmark = True\n",
    "    if torch.cuda.is_available() and args.use_gpu:\n",
    "        model = torch.nn.DataParallel(model).cuda()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "    criterion_aux=nn.NLLLoss(weight=None)\n",
    "    criterion_main=Multi_DiceLoss(class_num=args.num_classes)\n",
    "    criterion=[criterion_aux,criterion_main]\n",
    "    if mode=='train':\n",
    "        train(args, model, optimizer,criterion, dataloader_train, dataloader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0, lr 0.010000: 100%|██████████| 216/216 [00:54<00:00,  3.97it/s, loss=0.803320]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for train : 0.803320\n",
      "\n",
      "\n",
      "Start Validation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mean_D: 0.596929, Dice1: 0.597, Dice2: 0.000, Dice3: 0.000, ACC: 0.870: 100%|██████████| 25/25 [00:01<00:00, 18.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean_Dice: 0.5969288599799044\n",
      "Dice1: 0.5969295506846446\n",
      "Dice2: 0.0\n",
      "Dice3: 0.0\n",
      "Acc: 0.870302734375\n",
      "0.5969288599799044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1, lr 0.009095: 100%|██████████| 216/216 [00:49<00:00,  4.32it/s, loss=0.576403]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for train : 0.576403\n",
      "\n",
      "\n",
      "Start Validation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mean_D: 0.522542, Dice1: 0.523, Dice2: 0.000, Dice3: 0.000, ACC: 0.880: 100%|██████████| 25/25 [00:01<00:00, 19.77it/s]\n",
      "epoch 12, lr 0.008639:  41%|████      | 88/216 [26:15<38:11, 17.90s/it, loss=0.443743]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean_Dice: 0.5225420634556479\n",
      "Dice1: 0.5225428563920335\n",
      "Dice2: 0.0\n",
      "Dice3: 0.0\n",
      "Acc: 0.880234375\n",
      "0.5969288599799044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 2, lr 0.008181: 100%|██████████| 216/216 [00:50<00:00,  4.29it/s, loss=0.530844]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for train : 0.530844\n",
      "\n",
      "\n",
      "Start Validation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mean_D: 0.596884, Dice1: 0.597, Dice2: 0.000, Dice3: 0.000, ACC: 0.891: 100%|██████████| 25/25 [00:01<00:00, 22.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean_Dice: 0.5968835769862563\n",
      "Dice1: 0.5968843972820866\n",
      "Dice2: 0.0\n",
      "Dice3: 0.0\n",
      "Acc: 0.8907871791294643\n",
      "0.5969288599799044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 3, lr 0.007254: 100%|██████████| 216/216 [00:49<00:00,  4.33it/s, loss=0.497686]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for train : 0.497686\n",
      "\n",
      "\n",
      "Start Validation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mean_D: 0.627309, Dice1: 0.627, Dice2: 0.000, Dice3: 0.000, ACC: 0.900: 100%|██████████| 25/25 [00:01<00:00, 24.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean_Dice: 0.627308667741341\n",
      "Dice1: 0.6273095303083812\n",
      "Dice2: 0.0\n",
      "Dice3: 0.0\n",
      "Acc: 0.8998517717633928\n",
      "0.627308667741341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 4, lr 0.006314: 100%|██████████| 216/216 [00:56<00:00,  3.83it/s, loss=0.512156]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for train : 0.512156\n",
      "\n",
      "\n",
      "Start Validation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mean_D: 0.623836, Dice1: 0.624, Dice2: 0.000, Dice3: 0.000, ACC: 0.891: 100%|██████████| 25/25 [00:01<00:00, 20.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean_Dice: 0.6238356042336629\n",
      "Dice1: 0.6238364006072382\n",
      "Dice2: 0.0\n",
      "Dice3: 0.0\n",
      "Acc: 0.8910316685267857\n",
      "0.627308667741341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 5, lr 0.005359: 100%|██████████| 216/216 [00:48<00:00,  4.45it/s, loss=0.476493]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for train : 0.476493\n",
      "\n",
      "\n",
      "Start Validation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mean_D: 0.697054, Dice1: 0.697, Dice2: 0.000, Dice3: 0.000, ACC: 0.909: 100%|██████████| 25/25 [00:01<00:00, 22.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean_Dice: 0.697053829321663\n",
      "Dice1: 0.6970546781610041\n",
      "Dice2: 0.0\n",
      "Dice3: 0.0\n",
      "Acc: 0.9094635881696429\n",
      "0.697053829321663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 6, lr 0.004384: 100%|██████████| 216/216 [00:43<00:00,  4.92it/s, loss=0.449675]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for train : 0.449675\n",
      "\n",
      "\n",
      "Start Validation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mean_D: 0.679135, Dice1: 0.679, Dice2: 0.000, Dice3: 0.000, ACC: 0.910: 100%|██████████| 25/25 [00:01<00:00, 22.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean_Dice: 0.6791354873771182\n",
      "Dice1: 0.6791363698345356\n",
      "Dice2: 0.0\n",
      "Dice3: 0.0\n",
      "Acc: 0.9098067801339286\n",
      "0.697053829321663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 7, lr 0.003384: 100%|██████████| 216/216 [00:44<00:00,  4.87it/s, loss=0.461182]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for train : 0.461182\n",
      "\n",
      "\n",
      "Start Validation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mean_D: 0.706525, Dice1: 0.707, Dice2: 0.000, Dice3: 0.000, ACC: 0.914: 100%|██████████| 25/25 [00:01<00:00, 23.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean_Dice: 0.7065247169303409\n",
      "Dice1: 0.7065255934434017\n",
      "Dice2: 0.0\n",
      "Dice3: 0.0\n",
      "Acc: 0.9140677315848215\n",
      "0.7065247169303409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 8, lr 0.002349: 100%|██████████| 216/216 [00:52<00:00,  4.14it/s, loss=0.418760]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for train : 0.418760\n",
      "\n",
      "\n",
      "Start Validation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mean_D: 0.718410, Dice1: 0.718, Dice2: 0.000, Dice3: 0.000, ACC: 0.916: 100%|██████████| 25/25 [00:01<00:00, 20.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean_Dice: 0.7184102240469099\n",
      "Dice1: 0.7184110933260801\n",
      "Dice2: 0.0\n",
      "Dice3: 0.0\n",
      "Acc: 0.9156532505580357\n",
      "0.7184102240469099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 9, lr 0.001259: 100%|██████████| 216/216 [00:48<00:00,  4.43it/s, loss=0.410315]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for train : 0.410315\n",
      "\n",
      "\n",
      "Start Validation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mean_D: 0.704157, Dice1: 0.704, Dice2: 0.000, Dice3: 0.000, ACC: 0.919: 100%|██████████| 25/25 [00:01<00:00, 22.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean_Dice: 0.704157175036876\n",
      "Dice1: 0.704158107250182\n",
      "Dice2: 0.0\n",
      "Dice3: 0.0\n",
      "Acc: 0.9187862723214286\n",
      "0.7184102240469099\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    seed=1234\n",
    "    torch.manual_seed(seed)   # 固定初始化\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    args=DefaultConfig()\n",
    "    modes = 'train'\n",
    "    if modes=='train':\n",
    "        main(mode='train', args=args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start test!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|██████████| 107/107 [00:06<00:00, 17.33it/s]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    save_path = 'test'\n",
    "    model_path = 'checkpoints/checkpoint_latest.pth'\n",
    "    dataset_test = Data(os.path.join(DefaultConfig.data, 'test'), scale=(DefaultConfig.crop_width,\n",
    "                                                                         DefaultConfig.crop_height), mode='test')\n",
    "    args = DefaultConfig()\n",
    "    dataloader_test = DataLoader(\n",
    "        dataset_test,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        drop_last=False\n",
    "    )\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args.cuda\n",
    "    model_all = {'BaseNet': CPFNet(out_planes=args.num_classes)}\n",
    "    model = model_all[args.net_work]\n",
    "    cudnn.benchmark = True\n",
    "    if torch.cuda.is_available() and args.use_gpu:\n",
    "        model = torch.nn.DataParallel(model).cuda()\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    test(model, dataloader_test, args, save_path)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4905652b14e4b7eb92899b78ac499a22c488804455b27940a322fd82aaf71031"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
